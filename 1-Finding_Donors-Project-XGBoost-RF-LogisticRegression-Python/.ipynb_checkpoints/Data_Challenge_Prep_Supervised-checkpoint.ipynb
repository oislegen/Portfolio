{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data analysis\n",
    "\n",
    "# Check the versions of key python libraries\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: %s' % scipy.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: %s' % matplotlib.__version__)\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# statsmodels\n",
    "import statsmodels\n",
    "print('statsmodels: %s' % statsmodels.__version__)\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_squared_error \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "print('sklearn: %s' % sklearn.__version__)\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data visualization and exploration\n",
    "\n",
    "###########################################\n",
    "# Suppress matplotlib user warnings\n",
    "# Necessary for newer version of matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#\n",
    "# Display inline matplotlib plots with IPython\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "###########################################\n",
    "\n",
    "# Import libraries for data visualization\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Optional global parameter tuning for data visualization \n",
    "\n",
    "#from pylab import rcParams\n",
    "#matplotlib.rcParams['axes.labelsize'] = 14\n",
    "#matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "#matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "#matplotlib.rcParams['text.color'] = 'k'\n",
    "#rcParams['figure.figsize'] = 18, 8\n",
    "\n",
    "# Package for fast EDA\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Exploration and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the first five rows of the data set\n",
    "df = pd.read_csv('file_name.csv', header=?, index_col=?, squeeze=?)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data types and the number of non-null observations in the columns of the dataset.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the basic statistical characteristics of all features:\n",
    "df.describe(include=['object','int','float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the feature set:\n",
    "\n",
    "**BALANCE:** Numeric. Float. Total amount owed to the company. No missing values. Minimum value: 0, maximum value: 19043.13856. Mean: 1564.474828. Right-skewed. 0.9\\% of the values are zero.\n",
    "\n",
    "**BALANCE_FREQ:** Numeric. Float. Conjecture: The percentage of time there is a positive balance in the account. No missing values. Min: 0, Max: 1, Mean: 0.85. Left-skewed. 69.4\\% has value 1 and 0.9\\% has value 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer to replace data by mean, median, most frequent or constant\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative imputer to replace data \n",
    "# at each step, a feature column is designated as output y and the other feature columns are treated as inputs X.\n",
    "#A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. \n",
    "#This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. \n",
    "#The results of the final imputation round are returned.\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a polynomial function of a variable to the missing values of another variable\n",
    "\n",
    "fitting_log=np.polynomial.polynomial.Polynomial.fit(X_ynotnull,y_notnull,\n",
    "                                         deg=1)\n",
    "# See the intercept and slope if the linear fit\n",
    "intercept, slope=fitting_log.convert().coef\n",
    "print('intercept:',intercept,'slope:',slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the null values of minimum payments by slope*BALANCE+intercept\n",
    "y_null=slope*X_ynull+intercept    \n",
    "\n",
    "# Show the numner of missing MINIMUM_PAYMENTS left\n",
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a heatmap of Pearson correlations between all numeric features\n",
    "def correlation_matrix(dataset):\n",
    "    numeric_columns=dataset.select_dtypes(include=['int','float']).columns.values.tolist()\n",
    "    sns.heatmap(dataset[numeric_columns].corr(),cmap='viridis_r',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy\n",
    "def analyze_numeric_features(dataset,target):\n",
    "    \"\"\"\n",
    "    Identify and draw histograms for numeric features\n",
    "    \"\"\"\n",
    "    # Define the color and font size for the plot below\n",
    "    base_color = sns.color_palette()[0]\n",
    "    # Identify numeric features in the data set\n",
    "    numeric_columns=dataset.select_dtypes(include=['int','float']).columns.values.tolist()\n",
    "    numeric_column_number=len(numeric_columns) # Number of numeric features\n",
    "    axis_length=5 # Base axis length for each graph\n",
    "    total_y_axis_length=axis_length*numeric_column_number\n",
    "    f, ax = plt.subplots(numeric_column_number,2,figsize=(2.5*axis_length,total_y_axis_length))\n",
    "    for i, label in enumerate(numeric_columns):\n",
    "        extra=(dataset[label].max()-dataset[label].min())/14 \n",
    "        bin_edges=np.arange(dataset[label].min(), np.ceil(dataset[label].max())+extra, extra)\n",
    "        bin_idxs=pd.cut(dataset[label], bin_edges, include_lowest=True,labels=False).astype(int)\n",
    "        pts_per_bin=dataset.groupby(bin_idxs).size()\n",
    "        count_bins=pd.DataFrame(pts_per_bin[bin_idxs],columns=['count'])\n",
    "        num_var_weights=np.true_divide(target,count_bins['count'])\n",
    "        sns.distplot(dataset[label], bins=bin_edges, vertical=True, kde=False, hist_kws={'alpha':1}, ax= ax[i][0]) \n",
    "        ax[i,0].set_xlabel('Count')\n",
    "        ax[i,1].hist(x=dataset[label], bins=bin_edges, weights=num_var_weights, orientation='horizontal', color=base_color)\n",
    "        ax[i,1].set_xlabel('Mean Target') \n",
    "    f.tight_layout()\n",
    "    f.show()   \n",
    "    return                             \n",
    "\n",
    "\n",
    "def numeric_scatterplots(dataset):\n",
    "    \"\"\"\n",
    "    Draw scatter plots of all combinations of numeric features\n",
    "    \"\"\"\n",
    "    from itertools import combinations\n",
    "    base_color = sns.color_palette()[0]\n",
    "    numeric_columns=dataset.select_dtypes(include=['int','float']).columns.values.tolist()\n",
    "    numeric_column_number=len(numeric_columns) # Number of numeric features\n",
    "    axis_length=5 # Base axis length for each graph\n",
    "    total_y_axis_length=axis_length*len(list(combinations(numeric_columns,2)))\n",
    "    f, ax = plt.subplots(len(list(combinations(numeric_columns,2))),1,figsize=(1.5*axis_length,total_y_axis_length))\n",
    "    for i, (x,y) in enumerate(list(combinations(numeric_columns,2))):\n",
    "        sns.regplot(dataset[x],dataset[y], color=base_color, scatter_kws={'alpha':0.1}, ax=ax[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simpler\n",
    "\n",
    "def analyze_features(dataset):\n",
    "    \"\"\"\n",
    "    Identify and draw histograms for numeric features\n",
    "    \"\"\"\n",
    "    # Define the color and font size for the plot below\n",
    "    base_color = sns.color_palette()[0]\n",
    "    # Identify numeric features in the data set\n",
    "    numeric_columns=dataset.columns\n",
    "    numeric_column_number=len(numeric_columns) # Number of numeric features\n",
    "    axis_length=8 # Base axis length for each graph\n",
    "    total_y_axis_length=axis_length*numeric_column_number\n",
    "    f, ax = plt.subplots(numeric_column_number,1,figsize=(axis_length,total_y_axis_length))\n",
    "    for i, label in enumerate(numeric_columns):\n",
    "        extra=(dataset[label].max()-dataset[label].min())/14 \n",
    "        bin_edges=np.arange(dataset[label].min(), np.ceil(dataset[label].max())+extra, extra)\n",
    "        sns.distplot(dataset[label], bins=bin_edges, vertical=True, kde=False, hist_kws={'alpha':1}, ax= ax[i]) \n",
    "        ax[i].set_xlabel('Count')\n",
    "    f.tight_layout()\n",
    "    f.show()   \n",
    "    return                \n",
    "\n",
    "def numeric_scatterplots(dataset):\n",
    "    \"\"\"\n",
    "    Draw scatter plots of all combinations of numeric features\n",
    "    \"\"\"\n",
    "    from itertools import combinations\n",
    "    base_color = sns.color_palette()[0]\n",
    "    numeric_columns=dataset.select_dtypes(include=['int','float']).columns.values.tolist()\n",
    "    numeric_column_number=len(numeric_columns) # Number of numeric features\n",
    "    axis_length=5 # Base axis length for each graph\n",
    "    total_y_axis_length=axis_length*len(list(combinations(numeric_columns,2)))\n",
    "    f, ax = plt.subplots(len(list(combinations(numeric_columns,2))),1,figsize=(1.5*axis_length,total_y_axis_length))\n",
    "    for i, (x,y) in enumerate(list(combinations(numeric_columns,2))):\n",
    "        sns.regplot(dataset[x],dataset[y], color=base_color, scatter_kws={'alpha':0.1}, ax=ax[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for a detailed barchart which shows the categories of a variable sorted by mean target variable\n",
    "base_color = sns.color_palette()[0]\n",
    "group_means=country_income.groupby(['native-country']).mean()\n",
    "order=group_means.sort_values(['income'],ascending=False).index\n",
    "\n",
    "# Draw the barchart\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(7)\n",
    "ax=sns.barplot(x=country_income['income'], y=country_income['native-country'], order=order, color=base_color)\n",
    "ax.set_xlabel('Mean Target Variable (Income=1 if >=50K, 0 otherwise)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize skewed continuous features of original and transformed data\n",
    "from math import ceil\n",
    "def distribution(data, features,transformed=False):\n",
    "    \"\"\"\n",
    "    Visualization code for displaying skewed distributions of features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the color for the plot\n",
    "    base_color = sns.color_palette()[0]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(11,35))\n",
    "\n",
    "    # Skewed feature plotting\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = fig.add_subplot(8,2,i+1)\n",
    "        ax.hist(data[feature], bins = 25, color = base_color)\n",
    "        ax.set_title(\"%s Distribution\"%(feature), fontsize = 14)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Number of Records\")\n",
    "        ax.set_ylim((0, 2000))\n",
    "        ax.set_yticks([0, 500, 1000, 1500, 2000])\n",
    "        ax.set_yticklabels([0, 500, 1000, 1500, \">2000\"])\n",
    "\n",
    "    # Plot aesthetics\n",
    "    if transformed:\n",
    "        fig.suptitle(\"Log-transformed Distributions of Features\", \\\n",
    "            fontsize = 12, y = 1.03)\n",
    "    else:\n",
    "        fig.suptitle(\"Skewed Distributions of Features\", \\\n",
    "            fontsize = 12, y = 1.03)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the skewed features\n",
    "features_log_transformed = credit_card_cleaned.apply(lambda x: np.log(x + 1))\n",
    "\n",
    "# Visualize the new log distributions\n",
    "distribution(features_log_transformed, features_log_transformed.columns,transformed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize or standardize numerical features\n",
    "# Import sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "# scaler = StandardScaler()\n",
    "numerical = features_raw.select_dtypes(include=['int','float']).columns.values.tolist()\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
    "\n",
    "# Show 5 examples of records with scaling applied\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_features(dataset,target):\n",
    "    \"\"\"\n",
    "    Write a function to build bar charts for all categorical variables as well as an adapted barcharts showing how the mean           income changes for each label of the categorical variables\n",
    "    \"\"\"\n",
    "    # Define the color for the plot\n",
    "    base_color = sns.color_palette()[0]\n",
    "    # Identify categorical features in the data set\n",
    "    cat_columns=dataset.select_dtypes(include=['object']).columns.values.tolist()\n",
    "    cat_column_number=len(cat_columns) # Number of numeric features\n",
    "    axis_length=5 # Base axis length for each graph\n",
    "    total_y_axis_length=axis_length*cat_column_number\n",
    "    f, ax = plt.subplots(cat_column_number,2,figsize=(2.5*axis_length,total_y_axis_length))\n",
    "    for i, label in enumerate(cat_columns):\n",
    "        # Get the frequency order from high to low frequency for nominal variables \n",
    "        order= dataset[label].value_counts().index\n",
    "        sns.countplot(data=dataset, y=label, order=order, color=base_color, ax=ax[i][0])\n",
    "        sns.barplot(x=target, y=dataset[label], order=order, color=base_color, ax=ax[i][1])\n",
    "        ax[i,1].set_xlabel('Mean Income (Income=1 if >=50K, 0 otherwise)')\n",
    "    # Change the absolute frequency bar charts to relative frequency\n",
    "    n_points=dataset.shape[0]\n",
    "    j=0\n",
    "    for i in range(len(cat_columns)):   \n",
    "        xlimit=ax[i,j].get_xlim()[1]\n",
    "        limit=xlimit/n_points\n",
    "\n",
    "        # Generate tick mark locations and names\n",
    "        tick_props = np.arange(0, limit+0.3, 0.2)\n",
    "        tick_names = ['{:.1f}'.format(v) for v in tick_props]\n",
    "        ax[i,j].set_xticks(tick_props*n_points)\n",
    "        ax[i,j].set_xticklabels(tick_names)\n",
    "        ax[i,j].set_xlabel('Frequency')\n",
    "        \n",
    "        # Add annotations\n",
    "        ax[i,j].get_ylabel()\n",
    "        cat_counts = dataset[ax[i,j].get_ylabel()].value_counts()\n",
    "        max_count = dataset[ax[i,j].get_ylabel()].value_counts().max()\n",
    "        locs= ax[i,j].get_yticks() # get the current tick locations and labels\n",
    "        labels=ax[i,j].get_yticklabels()\n",
    "        \n",
    "        # Loop through each pair of locations and labels\n",
    "        for loc, label in zip(locs, labels):\n",
    "            # Get the text property for the label to get the correct count\n",
    "            count = cat_counts[label.get_text()]\n",
    "            pct_string = '{:0.1f}%'.format(100*count/n_points)\n",
    "            # Print the annotation just below the top of the bar\n",
    "            ax[i,j].text(count+(limit+0.25)*n_points*0.1, loc, pct_string, ha = 'center', color = 'black')\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.show() \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "\n",
    "# Encode the 'income_raw' data to numerical values\n",
    "income = (income_raw=='>50K').astype(int)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = features_final.columns.values.tolist()\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                    income, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1f4894a5b56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Naive predictor performance with all cases positive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincome\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data encoded to numerical values done in the data preprocessing step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mFP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;31m# Specific to the naive case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# No predicted negatives in the naive case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Naive predictor performance with all cases positive\n",
    "TP = np.sum(income) # Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data encoded to numerical values done in the data preprocessing step.\n",
    "FP = income.count() - TP # Specific to the naive case\n",
    "\n",
    "TN = 0 # No predicted negatives in the naive case\n",
    "FN = 0 # No predicted negatives in the naive case\n",
    "\n",
    "# TODO: Calculate accuracy, precision and recall\n",
    "accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "recall = TP/(TP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "\n",
    "# TODO: Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\n",
    "beta=0.5\n",
    "fscore =(1+beta**2)*precision*recall/((beta**2*precision)+recall) \n",
    "\n",
    "# Print the results \n",
    "print(\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and prediction pipeline\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    \n",
    "    \n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end-start\n",
    "        \n",
    "    # Get the predictions on the test set(X_test),\n",
    "    # then get predictions on the first 300 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "            \n",
    "    # Compute accuracy on the first 300 training samples which is y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n",
    "        \n",
    "    # Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # Compute F-score on the the first 300 training samples using fbeta_score()\n",
    "    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=0.5)\n",
    "        \n",
    "    # Compute F-score on the test set which is y_test\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)\n",
    "       \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-41dcd556100e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Calculate the number of samples for 1%, 10%, and 100% of the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msamples_100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0msamples_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msamples_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the models\n",
    "\n",
    "# Import the three supervised learning models from sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the three models\n",
    "clf_A = xgb.XGBClassifier(random_state=0)\n",
    "clf_B = RandomForestClassifier(random_state=0)\n",
    "clf_C = LogisticRegression(random_state=0)\n",
    "\n",
    "# Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "samples_100 = len(y_train)\n",
    "samples_10 = int(len(y_train)/10)\n",
    "samples_1 = int(len(y_train)/100)\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function for comparing the evaluation metrics of three different models\n",
    "\n",
    "def evaluate(results, accuracy, f1):\n",
    "    \"\"\"\n",
    "    Visualization code to display results of various learners.\n",
    "    \n",
    "    inputs:\n",
    "      - learners: a list of supervised learners\n",
    "      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n",
    "      - accuracy: The score for the naive predictor\n",
    "      - f1: The score for the naive predictor\n",
    "    \"\"\"\n",
    "  \n",
    "   \n",
    "    # Define the color and font size for the plot below\n",
    "    base_color1 = sns.color_palette()[0]\n",
    "    base_color2 = sns.color_palette()[1]\n",
    "    base_color3 = sns.color_palette()[2]\n",
    "    matplotlib.rcParams.update({'font.size': 12})\n",
    "    SMALL_SIZE = 10\n",
    "    MEDIUM_SIZE = 12\n",
    "    BIGGER_SIZE = 14\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(2, 3, figsize = (11,7))\n",
    "\n",
    "    # Constants\n",
    "    bar_width = 0.3\n",
    "    colors = [base_color1, base_color2,base_color3]\n",
    "    \n",
    "    # Super loop to plot four panels of data\n",
    "    for k, learner in enumerate(results.keys()):\n",
    "        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n",
    "            for i in np.arange(3):\n",
    "                \n",
    "                # Create plot code\n",
    "                ax[j//3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n",
    "                ax[j//3, j%3].set_xticks([0.45, 1.45, 2.45])\n",
    "                ax[j//3, j%3].set_xticklabels([\"Sample: 1%\", \"10%\", \"100%\"])\n",
    "                ax[j//3, j%3].set_xlim((-0.1, 3.0))\n",
    "    \n",
    "    # Add unique y-labels\n",
    "    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[0, 1].set_ylabel(\"Accuracy Score\")\n",
    "    ax[0, 2].set_ylabel(\"F-score\")\n",
    "    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[1, 1].set_ylabel(\"Accuracy Score\")\n",
    "    ax[1, 2].set_ylabel(\"F-score\")\n",
    "    \n",
    "    # Add horizontal lines for naive predictors\n",
    "    ax[0, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    \n",
    "    # Set y-limits for score panels\n",
    "    ax[0, 1].set_ylim((0, 1))\n",
    "    ax[0, 2].set_ylim((0, 1))\n",
    "    ax[1, 1].set_ylim((0, 1))\n",
    "    ax[1, 2].set_ylim((0, 1))\n",
    "    \n",
    "    #Add titles\n",
    "    ax[0, 0].set_title(\"Model Training\")\n",
    "    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n",
    "    ax[0, 2].set_title(\"F-score on Training Subset\")\n",
    "    ax[1, 0].set_title(\"Model Predicting\")\n",
    "    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n",
    "    ax[1, 2].set_title(\"F-score on Testing Set\")\n",
    "\n",
    "    # Create patches for the legend\n",
    "    patches = []\n",
    "    for i, learner in enumerate(results.keys()):\n",
    "        patches.append(mpatches.Patch(color = colors[i], label = learner))\n",
    "    plt.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n",
    "               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n",
    "    \n",
    "    # Aesthetics\n",
    "    plt.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter-tuning for the best model\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = xgb.XGBClassifier(random_state=0)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = {'max_depth': [1,2,3], 'learning_rate': [0.1,0.3,1], 'n_estimators':[300, 400, 500]}\n",
    "\n",
    "# Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer,cv=3)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to show the feature importances of features given a model and feature importances of that model\n",
    "def feature_plot(model, importances, X, num_features):\n",
    "    # Display the features in the order of importance\n",
    "    indices = np.argsort(importances)[:num_features]\n",
    "    columns = X.columns.values[indices[:]]\n",
    "    values = importances[indices][:]\n",
    "\n",
    "    # Define the colors for the plot\n",
    "    base_color1 = sns.color_palette()[0]\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize = (8,6))\n",
    "    plt.title(\"Feature Importances ({})\".format(model.__class__.__name__), fontsize = 16)\n",
    "    plt.barh(np.arange(num_features), values, align=\"center\", color = base_color1)\n",
    "    plt.yticks(np.arange(num_features), columns, fontsize = 14)\n",
    "    plt.xlabel(\"Normalized Weight\", fontsize = 14, fontweight='bold')\n",
    "    plt.ylabel(\"Features\", fontsize = 14, fontweight='bold')\n",
    "    #ax.set_yticklabels({'t-1':'# of calls one day ago','t-2':'# of calls two days ago','t-3':'# of calls three days ago','t-4':'# of calls four days ago', 't-5':'# of calls five days ago','t-6':'# of calls six days ago','t-7':'# of calls one week ago',\\\n",
    "    #                'observed_high':'highest temperature','observed_low':'lowest temperature'})\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a supervised learning model that has 'feature_importances_'\n",
    "# Will use three supervised learning models to compare the most important features \n",
    "# Already imported xgboost to use the supervised learning algorithm XGBClassifier for feature importance\n",
    "# Already imported random forest classifier \n",
    "# Import adaboost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Import functionality for cloning a model\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Train the supervised model on the training set using .fit(X_train, y_train)\n",
    "model1 = (clone(best_clf)).fit(X_train, y_train) # Use the clone of best_clf computed above using xgboost classifier\n",
    "model2=RandomForestClassifier(random_state=0)\n",
    "model2.fit(X_train, y_train) \n",
    "model3=AdaBoostClassifier(random_state=0)\n",
    "model3.fit(X_train, y_train) \n",
    "\n",
    "for model in [model1, model2, model3]:\n",
    "    # Extract the feature importances using .feature_importances_ \n",
    "    importances = model.feature_importances_ \n",
    "    # Plot feature importances for all features\n",
    "    feature_plot(model, importances, X, num_features=X.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
